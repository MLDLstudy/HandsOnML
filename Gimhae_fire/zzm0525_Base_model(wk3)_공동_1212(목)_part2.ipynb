{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/ieunpyo/anaconda3/envs/work/lib/python3.7/site-packages (from bayesian-optimization) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/ieunpyo/anaconda3/envs/work/lib/python3.7/site-packages (from bayesian-optimization) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/ieunpyo/anaconda3/envs/work/lib/python3.7/site-packages (from bayesian-optimization) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ieunpyo/anaconda3/envs/work/lib/python3.7/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp37-none-any.whl size=10032 sha256=444d9a35b93b71a507c7b5b38d9e91bbe29f2c018b903b7e2ce4e7994653e635\n",
      "  Stored in directory: /Users/ieunpyo/Library/Caches/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '/content/drive/work/gimhae_fire/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X = np.load('/Users/ieunpyo/PycharmProjects/Kaggle/gimhae_fire/Train_X.npy',allow_pickle=True)\n",
    "Test_X = np.load('/Users/ieunpyo/PycharmProjects/Kaggle/gimhae_fire/Test_X.npy',allow_pickle=True)\n",
    "Validation_X = np.load('/Users/ieunpyo/PycharmProjects/Kaggle/gimhae_fire/Validation_X.npy',allow_pickle=True)\n",
    "\n",
    "Train_y = np.load('/Users/ieunpyo/PycharmProjects/Kaggle/gimhae_fire/Train_y.npy',allow_pickle=True)\n",
    "Validation_y = np.load('/Users/ieunpyo/PycharmProjects/Kaggle/gimhae_fire/Validation_y.npy',allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59199, 317)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators : Number of trees in random forest\n",
    "#### max_features : The number of features to consider when looking for the best split\n",
    "#### max_depth : Maximum number of levels in tree\n",
    "#### min_samples_split : Minimum number of samples required to split a node\n",
    "#### min_samples_leaf : Minimum number of samples required at each leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second search\n",
    "##### hyperparameter 완전 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test_X & submission\n",
    "##### 정해진 hyperparameter로 N번 fitting 반복 predict -> f-score 최고점된 model로 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a17:56:26'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' +now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# n_estimators : Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 50, num = 10)]\n",
    "\n",
    "# max_features : The number of features to consider when looking for the best split\n",
    "max_features = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min_samples_split : Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# min_samples_leaf : Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 5, 10]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian optimization\n",
    "def bayesian_optimization(dataset, function, parameters):\n",
    "   X_train, y_train, X_test, y_test = dataset\n",
    "   n_iterations = 5\n",
    "   gp_params = {\"alpha\": 1e-4}\n",
    "\n",
    "   BO = BayesianOptimization(function, parameters)\n",
    "   BO.maximize(n_iter=n_iterations, **gp_params)\n",
    "\n",
    "   return BO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_optimization(cv_splits):\n",
    "    def function(n_estimators, max_depth, min_samples_split):\n",
    "        return cross_val_score(\n",
    "               RandomForestClassifier(\n",
    "                   n_estimators=int(max(n_estimators,0)),                                                               \n",
    "                   max_depth=int(max(max_depth,1)),\n",
    "                   min_samples_split=int(max(min_samples_split,2)), \n",
    "                   n_jobs=-1, \n",
    "                   random_state=42,   \n",
    "                   class_weight=\"balanced\"),  \n",
    "               X=X_train, \n",
    "               y=y_train, \n",
    "               cv=cv_splits,\n",
    "               scoring=\"roc_auc\",\n",
    "               n_jobs=-1).mean()\n",
    "\n",
    "    parameters = {\"n_estimators\": (10, 1000),\n",
    "                  \"max_depth\": (20, 150),\n",
    "                  \"min_samples_split\": (2, 10)}\n",
    "    \n",
    "    return function, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "def train(X_train, y_train, X_test, y_test, function, parameters):\n",
    "    dataset = (X_train, y_train, X_test, y_test)\n",
    "    cv_splits = 4\n",
    "    \n",
    "    best_solution = bayesian_optimization(dataset, function, parameters)      \n",
    "    params = best_solution[\"params\"]\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "             n_estimators=int(max(params[\"n_estimators\"], 0)),\n",
    "             max_depth=int(max(params[\"max_depth\"], 1)),\n",
    "             min_samples_split=int(max(params[\"min_samples_split\"], 2)), \n",
    "             n_jobs=-1, \n",
    "             random_state=42,   \n",
    "             class_weight=\"balanced\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
