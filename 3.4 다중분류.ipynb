{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중분류\n",
    "\n",
    "책에 쓰여있는대로, 여러가지 클래스를 구분하기 위해서 랜덤 포레스트나 나이브 베이즈, KNN 같이 애초에 여러개의 class를 구분할 수 있는 분류기를 쓸 수도 있지만, 이진 분류기를 반복연산하여 분류할 수도 있습니다. \n",
    "\n",
    "이 경우 OvO / OvR을 전략으로 채택할 수 있습니다. 책에도 써 있듯이, 전자는 각각의 조합마다 이진 분류기를 훈련시키는 방법을 말합니다. 0-1, 0-2 ... 9-7, 9-8 이런 식으로 각각의 분류기를 통과하고 나서, 가장 많이 채택된 값을 선택하는 방식으로 작동합니다. \n",
    "\n",
    "후자는 특정 숫자 하나만 구별하는 이진 분류기를 여러개 붙이는 방식으로 만들 수 있습니다. 가장 결정 점수가 높은 값을 반환하겠지요. 앞서 3.3에 등장하는 Never5classifier 10개를 붙여논 버전으로 이해하면 될 것 같습니다. OvO부터 살펴보겠습니다. 기본적으로 우리가 사용했던 mnist 데이터를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn 버전이 올라가면서, 책에 있는 fetch_mldata를 더 이상 지원하지 않게 되었습니다. 이제 mnist데이터를 가져오기 위해서 fetch_openml을 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(np.int8)\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 이렇게 가저오는 mnist data의 경우 y의 변수형이 조금 달라서 그냥 넣으면 분석이 안됩니다. astype에 넣어서 넘파이 데이터로 변환 후에 사용합니다. 아무튼 test데이터와 train데이터를 분류했습니다. 이제 OvO, OvR 를 생성해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OvO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier # SGDClassifier : 확률적 경사하강법을 사용하겠다는 선언.\n",
    "from sklearn.multiclass import OneVsOneClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적인 이진 분류기를 위해서 SGDClassifier를 불러왔습니다. sklearn.multiclass는 다중분류를 위한 하부 패키지입니다. 그 밑에는OneVsRestClassifier와 OneVsOneClassifier, OutputCodeClassifier가 있습니다. 마지막은 Class들을 0~1의 유클리드 공간에 사영한 뒤에, 그걸 학습하고, 예측은 학습한 것을 기반으로 어떻게 한다는데, 열심히 읽어보았지만, 도저히 이해가 안됩니다.\n",
    "\n",
    "일단 OvO부터 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                           class_weight=None,\n",
       "                                           early_stopping=False, epsilon=0.1,\n",
       "                                           eta0=0.0, fit_intercept=True,\n",
       "                                           l1_ratio=0.15,\n",
       "                                           learning_rate='optimal',\n",
       "                                           loss='hinge', max_iter=1000,\n",
       "                                           n_iter_no_change=5, n_jobs=None,\n",
       "                                           penalty='l2', power_t=0.5,\n",
       "                                           random_state=2014150099,\n",
       "                                           shuffle=True, tol=0.001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False),\n",
       "                   n_jobs=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state = 2014150099))\n",
    "ovo_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter : OneVsOneClassifier(estimator, n_jobs=None)\n",
    "\n",
    "기본적으로 갖는 parameter가 2개 밖에 없습니다. 앞에는 OvO방식으로 작동할 estimator를 넣어줍니다. 당연히 fit 메서드를 가지고 있어야 하며, 결과를 도출할 decision_function 메서드나 샘플의 클래스별 확률을 도출하는 predict_proba 중 하나를 가져야 합니다. \n",
    "\n",
    "n_jobs를 통해 프로세서의 코어 숫자(작업 개수)를 지정할 수 있습니다. 기본은 1이고, 어차피 CPU가 여러개가 아니면 늘려봐야 의미는 없습니다. -1을 지정하면 모든 작업을 총 동원하여 학습합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n",
      "[[ 1.66666719  5.33333144  0.66666789  7.33333253  4.33333187  2.66666791\n",
      "  -0.33333322  9.33333315  6.33332537  8.3333331 ]\n",
      " [ 1.66666719  7.33333296  3.66669361  8.33333293  3.66666749  5.33333267\n",
      "   0.66666687  0.66666689  9.33333315  4.33333156]\n",
      " [-0.33333314  3.33333101  6.33333244  7.33333288  5.3333321   5.33333229\n",
      "   0.66666683  1.66666713  9.33333316  7.33333288]\n",
      " [ 2.66666685 -0.33333326  3.66666731  0.6666679   9.33333322  1.66666803\n",
      "   5.33333282  6.33333312  7.33333283  8.33333314]\n",
      " [ 9.33333323  0.66666681  6.33333281  5.33333245 -0.3333332   7.333333\n",
      "   7.33333297  1.66666683  4.33333309  3.66666731]\n",
      " [ 8.33333292  1.66666684  4.33333078  3.66666747  6.33333259  2.66666721\n",
      "   9.33333326 -0.3333332   7.33333243  1.66666705]\n",
      " [ 1.6666669   1.66666776  2.66666847  5.33333249  8.33333303  2.66666714\n",
      "  -0.33333317  7.33333306  6.33333167  9.33333315]\n",
      " [ 2.66666683  0.6666668   1.66666695  9.33333323  4.33333284  8.33333313\n",
      "  -0.33333328  6.33333313  5.33333315  7.33333319]\n",
      " [ 0.66666686  7.33333304  3.3333318   7.33333296  3.66666791  6.33333297\n",
      "   1.66666689 -0.33333318  9.33333318  6.33333237]\n",
      " [-0.33333321  9.33333316  4.33331994  7.33333294  0.66666751  6.33333223\n",
      "   2.66666711  2.66666946  8.33333304  3.66666779]]\n",
      "[7 8 8 4 0 6 9 3 8 1]\n"
     ]
    }
   ],
   "source": [
    "print(ovo_clf.predict([X[36000]]))\n",
    "print(ovo_clf.decision_function(X_train[:10]))\n",
    "print(ovo_clf.predict(X_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__alpha': 0.0001, 'estimator__average': False, 'estimator__class_weight': None, 'estimator__early_stopping': False, 'estimator__epsilon': 0.1, 'estimator__eta0': 0.0, 'estimator__fit_intercept': True, 'estimator__l1_ratio': 0.15, 'estimator__learning_rate': 'optimal', 'estimator__loss': 'hinge', 'estimator__max_iter': 1000, 'estimator__n_iter_no_change': 5, 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__power_t': 0.5, 'estimator__random_state': 2014150099, 'estimator__shuffle': True, 'estimator__tol': 0.001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=2014150099, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False), 'n_jobs': None}\n",
      "===============================\n",
      "0.9448833333333333\n"
     ]
    }
   ],
   "source": [
    "print(ovo_clf.get_params(deep=True))\n",
    "print(\"===============================\")\n",
    "print(ovo_clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods :\n",
    "\n",
    "##### decision_fucntion(self, X)\n",
    "    OvO에서 X에 대해 사용한 decision_funciton을 반환합니다.\n",
    "\n",
    "##### fit(self,X,y)\n",
    "X,y에 대해 모델을 피팅합니다. \n",
    "\n",
    "##### partial_fit(self,X,y[,classes])\n",
    "지정해준 y class에 대해서만 피팅합니다 \n",
    "\n",
    "##### get_params(self, deep = True)\n",
    "파라미터들을 반환합니다. deep=True를 지정해주면 subobjects를 생성합니다.\n",
    "\n",
    "##### predict(self, X)\n",
    "피팅한 모델을 기반으로 X에 대해 예측합니다.\n",
    "\n",
    "##### score(self, X,y, sample_weight=None)\n",
    "mean accuracy를 반환합니다. sample_weight를 통해 가중치에 해당하는 array를 지정해줄 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(len(ovo_clf.estimators_))\n",
    "print((ovo_clf.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes :\n",
    "##### estimators_\n",
    "n(n-1)/2개의 estimator들의 리스트입니다. \n",
    "\n",
    "##### classes_\n",
    "class의 label을 포함하는 array입니다. \n",
    "\n",
    "##### pairwise_indices_\n",
    "estimator가 _pairwise attribute를 가진다면 estimator를 훈련하는데 썼던 sample의 indicies들의 리스트를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OvR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SGDClassifier(random_state = 2014150099))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                            class_weight=None,\n",
       "                                            early_stopping=False, epsilon=0.1,\n",
       "                                            eta0=0.0, fit_intercept=True,\n",
       "                                            l1_ratio=0.15,\n",
       "                                            learning_rate='optimal',\n",
       "                                            loss='hinge', max_iter=1000,\n",
       "                                            n_iter_no_change=5, n_jobs=None,\n",
       "                                            penalty='l2', power_t=0.5,\n",
       "                                            random_state=2014150099,\n",
       "                                            shuffle=True, tol=0.001,\n",
       "                                            validation_fraction=0.1, verbose=0,\n",
       "                                            warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr_clf.fit(X_train[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 옵션이 OvO와 동일합니다. 간단하게 값들만 보고 가도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[[-2430660.38570224 -1374895.82472558  -343654.43203806 -1271980.74408565\n",
      "  -1210287.78091764  -991202.9029372  -2228723.70687807   853247.45832273\n",
      "   -839769.94370653  -255615.04170723]\n",
      " [-2883222.75909643 -1862094.04907251  -772102.86970634  -918711.30577438\n",
      "  -2078019.49094479  -664581.64811572 -3116814.45375546 -2505855.99639001\n",
      "    305056.40265691  -879682.18294531]\n",
      " [-2842158.62924695 -1530468.64357482  -723284.27249715 -1127564.95779074\n",
      "  -2353566.24565518  -844430.60719311 -3714747.35440058 -1653014.75850683\n",
      "    822352.77677347  -897229.34996136]\n",
      " [-4155165.11659277 -5945917.31305457 -2849206.36360626 -1702408.07504932\n",
      "   1558529.34316371 -2111018.8745732  -2331077.47852065 -1955388.79016059\n",
      "   -439404.88342495 -1866716.16097477]\n",
      " [ 2461695.57214283 -4464676.55108476 -1136874.78927433  -593698.28584348\n",
      "  -4426942.96592059 -1099420.93174278 -1647302.698623   -2852427.68401251\n",
      "   -981109.50931963 -1793986.73334938]\n",
      " [-3779340.60827205 -3158495.42471582 -1994837.21946557 -2792113.12163394\n",
      "  -2819647.92949144 -2839712.62384268  1701041.97480196 -3822586.08612453\n",
      "  -2296557.2109792  -1367165.79393502]\n",
      " [-1656664.89863846 -2726711.72267431  -782636.57022481 -2227391.39171583\n",
      "    137448.05615914 -1487204.92893755 -3046959.94680435 -1235728.79469397\n",
      "  -1561072.43025132    39311.14293117]\n",
      " [-3780258.06769409 -6018846.65279457 -4646650.93967667  1112403.06169462\n",
      "  -2664462.45035685 -1472657.07766675 -4828059.96837339 -2560258.32175434\n",
      "  -1064489.99674083 -1485578.15254508]\n",
      " [-4490071.13982414 -2547139.53798688 -1492047.48423888 -1668046.24968472\n",
      "  -1504396.10009731  -886036.34233252 -2588699.00051128 -2697246.94826936\n",
      "   1038800.93911237 -1531338.34336131]\n",
      " [-5382879.67337629  1043618.77319145 -1218794.04957065 -1089585.1758127\n",
      "  -2118420.60842284  -522039.66705102 -2022871.04310036 -1210397.9236157\n",
      "    -57783.29217112  -425840.82115745]]\n",
      "[7 8 8 4 0 6 4 3 8 1]\n",
      "----------------------------\n",
      "[7 8 8 4 0 6 9 3 8 1]\n"
     ]
    }
   ],
   "source": [
    "print(ovr_clf.predict([X[36000]]))\n",
    "print(ovr_clf.decision_function(X_train[:10]))\n",
    "print(ovr_clf.predict(X_train[:10]))\n",
    "print(\"----------------------------\")\n",
    "print(ovo_clf.predict(X_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__alpha': 0.0001, 'estimator__average': False, 'estimator__class_weight': None, 'estimator__early_stopping': False, 'estimator__epsilon': 0.1, 'estimator__eta0': 0.0, 'estimator__fit_intercept': True, 'estimator__l1_ratio': 0.15, 'estimator__learning_rate': 'optimal', 'estimator__loss': 'hinge', 'estimator__max_iter': 1000, 'estimator__n_iter_no_change': 5, 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__power_t': 0.5, 'estimator__random_state': 2014150099, 'estimator__shuffle': True, 'estimator__tol': 0.001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=2014150099, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False), 'n_jobs': None}\n",
      "===============================\n",
      "0.8437333333333333\n"
     ]
    }
   ],
   "source": [
    "print(ovr_clf.get_params(deep=True))\n",
    "print(\"===============================\")\n",
    "print(ovr_clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(ovr_clf.estimators_))\n",
    "print(len(ovo_clf.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 각각이 연산한 estimator의 갯수가 다른 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "자체적으로 다중분류를 수행하는 RandomForest가 소개되어있어서 조사했습니다. sklearn.ensemble에서 임포트합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=2014150099,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state = 2014150099)\n",
    "forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter : (n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "뭐가 엄청 많습니다. 하지만 대부분 tree에 대한 옵션이고, 그마저도 죄다 optional이라 ()만 해도 알아서 디폴트로 잘 굴려줍니다.\n",
    "\n",
    "##### n_estimators = int\n",
    "트리의 개수를 정합니다. 디폴트는 10이나 0.22부터 100으로 증가됩니다. \n",
    "\n",
    "##### criterion = 'gini / entropy'\n",
    "평가 기준 함수를 정합니다. 둘 중하나를 선택할 수 있으며 지니가 디폴트입니다.\n",
    "\n",
    "##### min / max들\n",
    "tree에 대해서 여러가지 제한을 정합니다. 이 부분에 대한 설명은 \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "여길 참조하세요. \n",
    "\n",
    "##### bootstrap = True / False\n",
    "부트스트랩 데이터를 사용할지 여부를 정합니다. 기본으로 True며, False를 설정하면 그냥 전체 데이터를 사용합니다. \n",
    "\n",
    "##### random_state / n_jobs \n",
    "상동\n",
    "\n",
    "##### class_weight \n",
    "가중치를 정합니다. 디폴트는 없음이며, dictionary형태로 직접 입력하거나 balanced / balanced_subsample를 통해 자동으로 지정해줄 수 있습니다. 둘의 차이는 부트스트랩 대이터를 쓰지않거나, 쓰거나의 차이입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 8, 4, 0, 6, 9, 3, 8, 1], dtype=int8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf.predict(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba(X_train[:10])\n",
    "forest_clf.score(X_train,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods :\n",
    "##### decision_function(self, X),  fit(self,X,y), get_params(self, deep), partial_fit(self,X,y[,classes]), predict(self,X),  score(self, X,y[sample_weight]\n",
    "상동\n",
    "\n",
    "##### predict_proba(self, X)\n",
    "class에 대한 확률추정치를 반환합니다.\n",
    "\n",
    "#### Attributes:\n",
    "##### estimators_, classes_\n",
    "상동\n",
    "\n",
    "##### label_binarizer_\n",
    "binary를 multi로, multi를 binary로 변환하는데 쓰였던 object 반환\n",
    "\n",
    "##### multilabel_\n",
    "multi인지 여부를 논리값으로 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번외1) KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 여러가지 Multilabel classifier가 존재합니다. 책 뒤에서 다루는 SVM도 그 중 하나입니다. 이는 뒤에서 볼 거니까 위해 남기고 KNN과 Naive-Bayesian 두 가지만 더 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier() \n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 8 8 4 0 6 9 3 8 1]\n",
      "[7 8 8 4 0 6 9 3 8 1]\n"
     ]
    }
   ],
   "source": [
    "print(knn.predict(X_train[:10]))\n",
    "print(ovo_clf.predict(X_train[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_KNN은 예측하고 싶은 값의 주변 K개의 중에 가장 빈번한 값을 반환하는 단순한 알고리즘입니다. 하지만 생각보다 결과가 괜찮습니다. 위의 코드에서 ovo_clf와의 결과가 동일한 것을 확인할 수 있습니다. \n",
    "\n",
    "보다 구체적으로 KNN과 ovo_clf, ovr_clf, forest_clf 4가지를 비교해보겠습니다. x_test 앞의 1000개의 값을 예측시켜보고, y_test를 얼마나 잘 맞추는지 한 번 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test samples of OvO : 91\n",
      "Misclassified test samples of OvR : 176\n",
      "Misclassified test samples of Forest : 65\n",
      "Misclassified test samples of KNN : 39\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_ovo = ovo_clf.predict(X_test[:1000])\n",
    "y_test_pred_ovr = ovr_clf.predict(X_test[:1000])\n",
    "y_test_pred_forest = forest_clf.predict(X_test[:1000])\n",
    "y_test_pred_knn = knn.predict(X_test[:1000])\n",
    "\n",
    "print('Misclassified test samples of OvO : %d' %(y_test[:1000]!=y_test_pred_ovo).sum())\n",
    "print('Misclassified test samples of OvR : %d' %(y_test[:1000]!=y_test_pred_ovr).sum())\n",
    "print('Misclassified test samples of Forest : %d' %(y_test[:1000]!=y_test_pred_forest).sum())\n",
    "print('Misclassified test samples of KNN : %d' %(y_test[:1000]!=y_test_pred_knn).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "와우. 심지어 RandomForest보다도  결과가 좋군요. KNN은 이처럼 생각보다 좋은 결과를 반환하는 경우가 잦습니다. 이제 Parameter와 method들을 살펴봅시다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters: \n",
    "##### n_neighbors\n",
    "K의 숫자를 정합니다. 5를 기본값으로 가집니다.  \n",
    "##### weigth \n",
    "가중치를 정합니다. Uniform이 기본값이며 ‘distance’로 설정하면 거리의 역수로 가중치를 설정합니다. 즉 가까울수록 더 큰 가중치를 받습니다. \n",
    "##### metric \n",
    "거리 계산을 하는 방식을 지정합니다. 기본은 ‘minkowski’로 설정되어있습니다. minkowski 방식은 밑에 나오는 P와 함께 쓰입니다. \n",
    "##### P\n",
    "Minkowski의power parameter를 정합니다. 1이면 멘하탄 거리, 2이면 유클리드 거리로 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "predict(self, X) , predict_proba(self, X), Score(self, X,y[,sample_weight]) 등을 갖습니다. 반복되니 생략합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번외2) Naive Bayes\n",
    "\n",
    "나이브 베이즈는 조건부 확률 기반의 분류 방식입니다. 분류를 원하는 특성 변수에 대해 각각의 class에 대한 조건부 확률을 구하고, 그것들 간의 비를 통해 어떤 class일 확률이 더 높은지를 결정, 반환합니다. 이건 제가 이해하긴 했는데 수식 없이 글로는 설명이 힘드네요...톡방에 자료를 따로 공유하겠습니다. \n",
    "\n",
    "코드 보시죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 나이브 베이즈는 naive_bayes에서 import합니다. fit, partial_fit, predict와 predict_proba, predict_log_proba, score등을 메서드로 가집니다. 위에서 다 본 것들이니 생략합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 8 8 4 0 6 9 3 8 1]\n",
      "[7 8 8 4 0 6 9 3 8 1]\n",
      "Misclassified test samples of MNB : 189\n"
     ]
    }
   ],
   "source": [
    "print(mnb.predict(X_train[:10]))\n",
    "print(ovo_clf.predict(X_train[:10]))\n",
    "\n",
    "y_test_pred_mnb = mnb.predict(X_test[:1000])\n",
    "print('Misclassified test samples of MNB : %d' %(y_test[:1000]!=y_test_pred_mnb).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생각보다 결과가 좋지는 않네요. Naive Bayes는 독립성과 조건부 x의 정규성을 임의로 가정하는 모델이기 때문에, 이 가정에서 벗어나는 데이터일수록 좋지 못한 결과를 반환합니다. parameter를 보고 마무리하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "##### alpha \n",
    "smoothing을 얼마나 할지를 지정합니다. 커질수록 smooth해집니다. 1이 기본입니다. \n",
    "\n",
    "##### fit_prior\n",
    "prior prop을 사용하지 여부를 boolean으로 입력합니다. 만일 아니라면 그냥 다 동등하게 간주하여 class를 학습합니다. 기본은 True입니다. \n",
    "\n",
    "##### class_prior \n",
    "class들의 Prior probabilities를 넘파이 어레이 형태로 입력합니다. 입력하지 않으면 데이터 기반으로 추정하지만, 이걸 지정해주면 자의로 정할 수도 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
